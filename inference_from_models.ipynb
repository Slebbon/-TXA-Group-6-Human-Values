{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r w2v_model\n",
    "# from word2vec.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r doc2vec_final\n",
    "# from doc2vec.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r lda\n",
    "# from nltk_coll_topic.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r lda_vectorizer\n",
    "# from nltk_coll_topic.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    # set lowercase\n",
    "    processed_text = [token.text.lower()for token in doc]\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_preprocess(example, model):\n",
    "    ex_tok = preprocess_text(example)\n",
    "    # create the vectors from the w2v model\n",
    "    ex_vectors = []\n",
    "\n",
    "    for word in ex_tok:\n",
    "        if word in w2v_model.wv:\n",
    "            ex_vectors.append(w2v_model.wv[word])\n",
    "        else:\n",
    "            ex_vectors.append(np.zeros(150)) #if the word is not in the model i append an array of zeros equal to the embeddings dimension, that is 150\n",
    "        \n",
    "    return ex_vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2v_topic_preprocess(example, d2v_model, topic_model, topic_vectorizer):\n",
    "\n",
    "    # first we extract the topics\n",
    "    ex_tf = topic_vectorizer.tranform(example)\n",
    "    ex_topics = topic_model.transform(ex_tf)\n",
    "\n",
    "    # we then extract the d2v vectors\n",
    "    ex_tok = preprocess_text(example)\n",
    "    ex_d2v = d2v_model.infer_vector(ex_tok)\n",
    "\n",
    "    # we now concatenate them\n",
    "    ex_d2v_topic_vec = np.array(np.concatenate(ex_topics, ex_d2v))\n",
    "    \n",
    "    return ex_d2v_topic_vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\cate9\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "# load the model\n",
    "lstm_model = load_model('lstm_model.h5')\n",
    "cnn_bce_model = load_model(\"cnn_bce_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r input\n",
    "# from undersampling.ipynb, a list of vectors obtained by concatenating for each document its doc2vec vector and a vector with the topic modeling probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we fit the scaler for the cnn on the training set\n",
    "X_train = np.asarray(input)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference(example, model_type):\n",
    "\n",
    "    if model_type == \"lstm\":\n",
    "        ex_vectors = w2v_preprocess(example, w2v_model)\n",
    "        ex_vectors = np.array(ex_vectors)\n",
    "        # setting max_len from the training set\n",
    "        max_len = 163\n",
    "        X_ex = pad_sequences(ex_vectors, max_len)\n",
    "        y_pred = lstm_model.predict(X_ex)\n",
    "\n",
    "    elif model_type == \"cnn\":\n",
    "        ex_vectors = d2v_topic_preprocess(example, doc2vec_final, lda, lda_vectorizer)\n",
    "        ex_vectors = np.array(ex_vectors)\n",
    "        X_ex = scaler.transform(ex_vectors)\n",
    "        # Reshape 'X_ex' to add an extra dimension\n",
    "        X_ex = X_ex.reshape((1, 1, X_ex.shape[0]))\n",
    "        y_pred = cnn_bce_model.predict(X_ex)\n",
    "\n",
    "    # we now turn the predictions in binary labels\n",
    "    threshold = 0.5\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    return y_pred_binary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
